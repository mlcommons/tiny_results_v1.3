# Tiny MLPerf Example Directory structure

A submission is for one code base for the benchmarks submitted. An org may make multiple submissions. A submission should take the form of a directory with the following structure. The structure must be followed regardless of the actual location of the actual code, e.g. in the MLPerf repo or an external code host site.

In case of submission of results for multiple systems, please use <system_desc.id> to differentiate these. System names may be arbitrary. We recognize implementatations for multiple systems of the same organization could have different dependencies on a common code base and on each other. When submitting the code, please organize the code as much as possible following a logical structure that makes it possible to reproduce the results, and accompany it with scripts and a README that explains the process. You can use multiple <implementation_id>s to structure your submission. While we do not have many examples in Tiny yet, you can refer to https://github.com/mlcommons/inference_results_v1.1/tree/main/closed or https://github.com/mlcommons/submissions_tiny_v1.3/tree/ref_submission/closed/mlperf_org for inspiration.

```
<division>
└── <submitting_organization>
    ├── systems
    │   ├── <system_desc_id>.json #combines hardware and software stack information (one file for each system benchmarked)
    │   ├── TinyMLPerf_v1.1_Submission_Checklist.pdf
    │   └── Energy-Hookup.pdf #image or text description how to reproduce energy configuration and measurment if submitting energy results
    ├── code
    │   └── <benchmark_name per reference>
    │       └── <implementation_id>
    │           └── <Code interface with runner and other arbitrary stuff>
    └── results
        └── <system_desc_id> # (one folder for each system benchmarked)
            └── <benchmark>
                ├── performance
                │   ├── EEMBC_RUNNER # Indicate to checker that the EEMBC runner was used (not needed for MLC runner)
                │   ├── result.txt #results summary produced by runner after performance test                
                │   ├── log.txt #log produced by runner after performance test                
                │   ├── script.async #script file produced by runner after performance test, if using EEMBC runner
                │   └── results.json # Detailed results if using MLC runner
                ├── accuracy
                │   ├── EEMBC_RUNNER # Indicate to checker that the EEMBC runner was used (not needed for MLC runner)
                │   ├── result.txt #results summmary produced by runner after accuracy test
                │   ├── log.txt #log produced by runner after accuracy test
                │   ├── script.async #script file produced by runner after accuracy test, if using EEMBC runner
                │   └── results.json # Detailed results if using MLC runner                
                └── energy #if submitting energy results
                    ├── EEMBC_RUNNER # Indicate to checker that the EEMBC runner was used (not needed for MLC runner)
                    ├── result.txt #results summmary produced by runner after accuracy test
                    ├── log.txt #log produced by runner after accuracy test
                    ├── script.async #script file produced by runner after accuracy test, if using EEMBC runner
                    └── results.json # Detailed results if using MLC runner                    
```


System names and implementation names may be arbitrary.

<division> must be one of {closed, open}.

<benchmark> must be one of {vww, ic, kws, ad, sww}.

Here is the list of mandatory files for all submissions in any division/category. However, your submission should still include all software information and related information for results replication.

* performance/result.txt
* performance/log.txt
* performance/script.async (if using EEMBC runner)
* performance/results.json (if using MLC runner)
* performance/EEMBC_RUNNER (if using EEMBC runner)
* accuracy/results.txt
* accuracy/log.txt
* accuracy/script.async  (if using EEMBC runner)
* accuracy/results.json  (if using MLC runner)
* accuracy/EEMBC_RUNNER  (if using EEMBC runner)

* calibration or weight transformation related code if the original MLPerf models are not used
* actual models if the models are not deterministically generated
* READMEs to enable users to replicate performance results
* code which interfaces with the runner
* <system_desc_id>.json, with name matching the name of the folder under results
* TinyMLPerf_v1.3_Submission_Checklist.pdf
